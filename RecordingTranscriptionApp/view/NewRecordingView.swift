import SwiftUI
import CoreLocation

struct NewRecordingView: View {
    enum RecordingState {
        case idle, recording, paused, finished, optimizingTranscription
    }

    // æµ‹è¯•æ¨¡å¼å¼€å…³ - ç”¨äºç»•è¿‡æƒé™æ£€æŸ¥æµ‹è¯•UIçŠ¶æ€
    @State private var isTestMode: Bool = false // æ”¹ä¸ºfalseå¯ç”¨çœŸå®å½•åˆ¶
    
    @State private var recordingState: RecordingState = .idle
    @State private var showFinishAlert = false
    @State private var showTranscriptionView = false
    @State private var transcriptText: String = ""
    @State private var translatedText: String = ""
    @State private var transcriptWordCount: Int = 0
    @StateObject private var permissionManager = MicrophonePermissionManager()
    @StateObject private var audioRecordingManager = AudioRecordingManager()
    @StateObject private var realtimeTranscriptionManager = RealTimeTranscriptionManager()
    @EnvironmentObject var sessionManager: RecordingSessionManager
    
    // é”™è¯¯å¤„ç†
    @State private var showErrorAlert = false
    
    var body: some View {
        VStack(spacing: 0) {
                // æµ‹è¯•æ¨¡å¼æŒ‡ç¤ºå™¨
                if isTestMode {
                    VStack(spacing: 8) {
                        HStack {
                            Image(systemName: "testtube.2")
                                .foregroundColor(.orange)
                                                    Text("Test Mode - Permission Check Bypassed")
                            .font(.caption)
                            .foregroundColor(.orange)
                        Spacer()
                        Button("Disable Test Mode") {
                            isTestMode = false
                        }
                            .font(.caption)
                            .foregroundColor(.blue)
                        }
                        
                        // å¿«é€Ÿæµ‹è¯•æŒ‰é’®
                        HStack(spacing: 8) {
                            Button("Test Optimization") {
                                testOptimizingState()
                            }
                            .font(.caption)
                            .padding(.horizontal, 8)
                            .padding(.vertical, 4)
                            .background(Color.blue.opacity(0.2))
                            .cornerRadius(4)
                            
                            Button("Test Translation") {
                                testTranslationView()
                            }
                            .font(.caption)
                            .padding(.horizontal, 8)
                            .padding(.vertical, 4)
                            .background(Color.green.opacity(0.2))
                            .cornerRadius(4)
                            
                            Button("Reset") {
                                resetRecording()
                            }
                            .font(.caption)
                            .padding(.horizontal, 8)
                            .padding(.vertical, 4)
                            .background(Color.gray.opacity(0.2))
                            .cornerRadius(4)
                        }
                    }
                    .padding(.horizontal, 16)
                    .padding(.vertical, 8)
                    .background(Color.orange.opacity(0.1))
                }
                
                // æƒé™æ£€æŸ¥ - æµ‹è¯•æ¨¡å¼ä¸‹è·³è¿‡
                if !isTestMode && !permissionManager.isPermissionGranted {
                    if permissionManager.isPermissionDenied {
                        PermissionDeniedView()
                    } else {
                        MicrophonePermissionView(onPermissionRequested: {
                            permissionManager.requestPermission()
                        })
                    }
                } else {
                    recordingInterface
                }
            }
                    .onAppear {
            permissionManager.checkPermissionStatus()
        }
        .onChange(of: audioRecordingManager.errorMessage) { _, errorMessage in
            if errorMessage != nil {
                showErrorAlert = true
            }
        }
        .alert("Recording Error", isPresented: $showErrorAlert) {
            Button("OK") {
                audioRecordingManager.clearError()
            }
        } message: {
            Text(audioRecordingManager.errorMessage ?? "Unknown error occurred")
        }
    }
    
    private var recordingInterface: some View {
        VStack(spacing: 0) {
            Spacer()
            // è®¡æ—¶å™¨åŒºåŸŸ
            timerArea
            // æ³¢å½¢å›¾åŒºåŸŸ
            waveformArea
            // ä¸»æŒ‰é’®åŒºåŸŸ
            mainButtonArea
            // è¾…åŠ©æŒ‰é’®åŒºåŸŸ
            auxiliaryButtonArea
            Spacer()
        }
        .padding(.horizontal)
        .overlay(transcriptionOverlay)
        .navigationTitle("New Recording")
        .alert(isPresented: $showFinishAlert, content: finishAlert)
        .overlay(optimizationOverlay)
        .sheet(isPresented: $showTranscriptionView, content: transcriptionSheet)
    }
    
    @ViewBuilder
    private var timerArea: some View {
        Group {
            if recordingState != .idle {
                Text(audioRecordingManager.formatDuration(audioRecordingManager.currentRecordingDuration))
                    .font(.largeTitle)
                    .monospacedDigit()
                    .frame(height: 40)
            } else {
                Color.clear.frame(height: 40)
            }
        }
        .padding(.bottom, 32)
    }
    
    @ViewBuilder
    private var waveformArea: some View {
        Group {
            if recordingState == .recording {
                RealTimeWaveformView(
                    audioLevels: audioRecordingManager.audioLevels,
                    isRecording: audioRecordingManager.isRecording && !audioRecordingManager.isPaused
                )
                .frame(height: 40)
                .padding(.horizontal)
            } else if recordingState == .paused {
                StaticWaveformView()
                    .frame(height: 40)
                    .padding(.horizontal)
            } else {
                Color.clear.frame(height: 40)
                    .padding(.horizontal)
            }
        }
        .padding(.bottom, 32)
    }
    
    @ViewBuilder
    private var mainButtonArea: some View {
        ZStack {
            Button(action: {
                handleRecordingButton()
            }) {
            ZStack {
                Circle()
                        .fill(buttonColor)
                        .frame(width: 88, height: 88)
                    Image(systemName: buttonIcon)
                        .font(.system(size: 64))
                        .foregroundColor(.white)
                }
            }
            .disabled(recordingState == .finished)
        }
        .frame(height: 100)
        .padding(.bottom, 32)
    }
    
    private var buttonColor: Color {
        switch recordingState {
        case .recording: return .red
        case .paused: return .orange
        case .finished: return .green
        default: return .blue
        }
    }
    
    private var buttonIcon: String {
        switch recordingState {
        case .recording: return "pause.circle.fill"
        case .paused: return "play.circle.fill"
        case .finished: return "checkmark.circle.fill"
        default: return "mic.circle.fill"
        }
    }
    
    @ViewBuilder
    private var auxiliaryButtonArea: some View {
        Group {
            if recordingState == .recording || recordingState == .paused {
                Button(action: {
                    finishRecording()
                }) {
                    ZStack {
                        Circle()
                            .fill(Color.gray)
                            .frame(width: 72, height: 72)
                        Image(systemName: "stop.circle.fill")
                            .font(.system(size: 48))
                            .foregroundColor(.white)
                    }
                }
            } else if recordingState == .finished {
                Button(action: {
                    resetRecording()
                }) {
                    ZStack {
                        Circle()
                            .fill(Color.white)
                            .frame(width: 72, height: 72)
                        Image(systemName: "arrow.clockwise.circle.fill")
                            .font(.system(size: 48))
                            .foregroundColor(.blue)
                    }
                }
            } else {
                Color.clear.frame(width: 72, height: 72)
            }
        }
        .frame(height: 80)
    }
    
    @ViewBuilder
    private var transcriptionOverlay: some View {
        VStack {
            if recordingState == .recording || recordingState == .paused {
                transcriptionStatusArea
            }
            
            Spacer()
            
            transcriptionIndicator
        }
    }
    
    @ViewBuilder
    private var transcriptionStatusArea: some View {
        VStack(spacing: 8) {
            if !realtimeTranscriptionManager.currentTranscript.isEmpty {
                Text(realtimeTranscriptionManager.currentTranscript)
                    .font(.caption)
                    .multilineTextAlignment(.leading)
                    .padding(8)
                    .background(Color.blue.opacity(0.1))
                    .cornerRadius(8)
            }
            if realtimeTranscriptionManager.isTranscribing {
                HStack {
                    Circle()
                        .fill(Color.red)
                        .frame(width: 8, height: 8)
                        .scaleEffect(1.5)
                        .opacity(0.8)
                        .animation(.easeInOut(duration: 1).repeatForever(autoreverses: true), value: realtimeTranscriptionManager.isTranscribing)
                    Text("Transcribing...")
                        .font(.caption2)
                        .foregroundColor(.secondary)
                    Spacer()
                }
            }
        }
        .padding(.horizontal, 16)
        .padding(.top, 16)
    }
    
    @ViewBuilder
    private var transcriptionIndicator: some View {
        HStack {
            Spacer()
            TranscriptionStatusIndicator(
                recordingState: recordingState,
                transcriptWordCount: realtimeTranscriptionManager.realtimeSegments.count,
                action: {
                    switch recordingState {
                    case .recording, .paused:
                        showTranscriptionView = true
                    case .finished:
                        if !sessionManager.sessions.isEmpty {
                            showTranscriptionView = true
                        }
                    case .idle, .optimizingTranscription:
                        break
                    }
                }
            )
            .padding(.trailing, 24)
            .padding(.bottom, 32)
        }
    }
    
    private func finishAlert() -> Alert {
        Alert(
            title: Text("Recording Finished"),
            message: Text("Do you want to confirm and save the recording?"),
            primaryButton: .default(Text("Confirm & Save"), action: {
                print("ğŸš¨ Alert Confirm & Save button pressed!")
                saveRecording()
            }),
            secondaryButton: .cancel(Text("Continue Editing"), action: {
                print("ğŸš¨ Alert Continue Editing button pressed!")
            })
        )
    }
    
    @ViewBuilder
    private var optimizationOverlay: some View {
        Group {
            if recordingState == .optimizingTranscription {
                ZStack {
                    Color.black.opacity(0.3)
                        .ignoresSafeArea()
                    
                    VStack(spacing: 16) {
                        ProgressView()
                            .scaleEffect(1.2)
                            .tint(.blue)
                        
                        Text("Optimizing Transcription...")
                            .font(.headline)
                            .foregroundColor(.primary)
                        
                        Text("Almost done")
                            .font(.caption)
                            .foregroundColor(.secondary)
                    }
                    .padding(24)
                    .background(Color(.systemBackground))
                    .cornerRadius(16)
                    .shadow(radius: 10)
                }
            }
        }
    }
    
    @ViewBuilder
    private func transcriptionSheet() -> some View {
        switch recordingState {
        case .recording, .paused:
            RealTimeTranscriptionView(manager: realtimeTranscriptionManager)
        case .finished:
            if let latestSession = sessionManager.sessions.first {
                RecordingPlaybackView(
                    recordingName: latestSession.name,
                    audioURL: URL(fileURLWithPath: latestSession.filePath),
                    sessionId: latestSession.id
                )
                .environmentObject(sessionManager)
            } else {
                TranscriptionTranslationView(
                    transcriptText: transcriptText,
                    translatedText: translatedText
                )
            }
        default:
            Text("No transcription available")
                .foregroundColor(.secondary)
        }
    }

    private var buttonTitle: String {
        switch recordingState {
        case .idle: return "Start Recording"
        case .recording: return "Pause Recording"
        case .paused: return "Resume Recording"
        case .finished: return "Finished"
        case .optimizingTranscription: return "Processing..."
        }
    }

    private func handleRecordingButton() {
        switch recordingState {
        case .idle:
            startRecording()
        case .recording:
            pauseRecording()
        case .paused:
            resumeRecording()
        case .finished:
            break
        case .optimizingTranscription:
            break // ä¼˜åŒ–è¿‡ç¨‹ä¸­ä¸å…è®¸æ“ä½œ
        }
    }

    private func startRecording() {
        // æµ‹è¯•æ¨¡å¼ä¸‹è·³è¿‡æƒé™æ£€æŸ¥
        if !isTestMode {
            // å†æ¬¡æ£€æŸ¥æƒé™
            guard permissionManager.isPermissionGranted else {
                permissionManager.checkPermissionStatus()
                return
            }
        }
        
        // å¯åŠ¨çœŸå®å½•åˆ¶
        audioRecordingManager.startRecording()
        recordingState = .recording
        
        // å¯åŠ¨å®æ—¶è½¬å†™
        Task {
            do {
                try await realtimeTranscriptionManager.startRealTimeTranscription()
                print("âœ… Real-time transcription started with recording")
            } catch {
                print("âŒ Failed to start real-time transcription: \(error)")
            }
        }
    }

    private func pauseRecording() {
        audioRecordingManager.pauseRecording()
        recordingState = .paused
        
        // æš‚åœå®æ—¶è½¬å†™
        realtimeTranscriptionManager.pauseRealTimeTranscription()
    }

    private func resumeRecording() {
        audioRecordingManager.resumeRecording()
        recordingState = .recording
        
        // æ¢å¤å®æ—¶è½¬å†™
        do {
            try realtimeTranscriptionManager.resumeRealTimeTranscription()
        } catch {
            print("âŒ Failed to resume real-time transcription: \(error)")
        }
    }

    private func finishRecording() {
        print("ğŸ›‘ finishRecording() called")
        audioRecordingManager.stopRecording()
        recordingState = .optimizingTranscription
        
        // åœæ­¢å®æ—¶è½¬å†™
        realtimeTranscriptionManager.stopRealTimeTranscription()
        
        // æ¨¡æ‹Ÿè½¬å†™ä¼˜åŒ–è¿‡ç¨‹ (2-3ç§’)
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.5) {
            print("ğŸ¯ Setting recordingState to .finished and showing alert")
            recordingState = .finished
            showFinishAlert = true
        }
    }

    private func resetRecording() {
        // å¦‚æœæ­£åœ¨å½•åˆ¶ï¼Œå…ˆåœæ­¢
        if audioRecordingManager.isRecording {
            audioRecordingManager.stopRecording()
        }
        
        recordingState = .idle
        showFinishAlert = false
    }
    
    private func saveRecording() {
        // è·å–å½•éŸ³ä¿¡æ¯è¿›è¡Œè°ƒè¯•
        let recordingInfo = audioRecordingManager.getCurrentRecordingInfo()
        print("ğŸ™ï¸ Saving recording:")
        print("   URL: \(recordingInfo.url?.absoluteString ?? "nil")")
        print("   Name: \(recordingInfo.name)")
        print("   Duration: \(recordingInfo.duration)")
        
        // æ£€æŸ¥æ˜¯å¦æœ‰å®æ—¶è½¬å†™æ•°æ®
        let hasRealtimeData = !realtimeTranscriptionManager.realtimeSegments.isEmpty
        print("ğŸ”„ Real-time transcription data: \(hasRealtimeData ? "Available (\(realtimeTranscriptionManager.realtimeSegments.count) segments)" : "None")")
        
        if hasRealtimeData {
            // ä½¿ç”¨å®æ—¶è½¬å†™æ•°æ®ä¿å­˜ä¼šè¯
            let realtimeSegments = realtimeTranscriptionManager.exportRealtimeSegments()
            sessionManager.addSessionWithRealTimeTranscription(
                from: audioRecordingManager,
                realtimeSegments: realtimeSegments
            )
            print("âœ… Session saved with real-time transcription data")
        } else {
            // ä½¿ç”¨ä¼ ç»Ÿæ–¹å¼ä¿å­˜
            if recordingInfo.url == nil {
                print("âš ï¸ No recording URL found, creating test session")
                let testSession = RecordingSession(
                    name: "Test Recording \(Date().timeIntervalSince1970)",
                    duration: recordingInfo.duration > 0 ? recordingInfo.duration : 8.0,
                    date: Date(),
                    fileSize: "1.2 MB",
                    sessionStatus: .completed,
                    sourceLanguage: "English",
                    targetLanguage: "Chinese",
                    hasTranslation: false,
                    priority: .normal,
                    sessionType: .memo,
                    filePath: "/test/path/recording.m4a",
                    wordCount: Int(recordingInfo.duration * 2.5),
                    transcriptionQuality: .good
                )
                sessionManager.sessions.insert(testSession, at: 0)
                print("âœ… Test session added. Total sessions: \(sessionManager.sessions.count)")
            } else {
                // ä¿å­˜å½•éŸ³åˆ°ä¼šè¯ç®¡ç†å™¨
                sessionManager.addSession(from: audioRecordingManager)
                print("âœ… Session saved with traditional method")
            }
        }
        
        print("ğŸ“ Sessions count after adding: \(sessionManager.sessions.count)")
        
        // é‡ç½®å½•éŸ³çŠ¶æ€å¹¶æ¸…ç©ºå®æ—¶è½¬å†™æ•°æ®ï¼ˆå› ä¸ºå·²ç»ä¿å­˜ï¼‰
        resetRecording()
        realtimeTranscriptionManager.clearSegments() // æ¸…ç©ºå·²ä¿å­˜çš„æ•°æ®
        
        // æ˜¾ç¤ºä¿å­˜æˆåŠŸçš„åé¦ˆï¼ˆå¯é€‰ï¼‰
        // è¿™é‡Œå¯ä»¥æ·»åŠ ä¸€ä¸ªToastæ¶ˆæ¯æˆ–è€…å…¶ä»–UIåé¦ˆ
    }


    
    // MARK: - æµ‹è¯•å‡½æ•°
    private func testOptimizingState() {
        // æ¨¡æ‹Ÿå½•éŸ³è¿‡ç¨‹
        startRecording()
        
        // 2ç§’åè¿›å…¥ä¼˜åŒ–çŠ¶æ€
        DispatchQueue.main.asyncAfter(deadline: .now() + 2.0) {
            finishRecording()
        }
    }
    
    private func testTranslationView() {
        // ç›´æ¥è·³è½¬åˆ°è½¬å†™ç¿»è¯‘ç•Œé¢
        recordingState = .finished
        showTranscriptionView = true
    }
    
    private func testSaveRecording() {
        // ç›´æ¥æµ‹è¯•ä¿å­˜åŠŸèƒ½
        saveRecording()
    }
}



// MARK: - å®æ—¶è½¬å†™çŠ¶æ€æŒ‡ç¤ºå™¨ç»„ä»¶
struct TranscriptionStatusIndicator: View {
    let recordingState: NewRecordingView.RecordingState
    let transcriptWordCount: Int
    let action: () -> Void
    
    @State private var isAnimating = false
    
    var body: some View {
        Button(action: action) {
            ZStack {
                // èƒŒæ™¯åœ†å½¢
                Circle()
                    .fill(statusColor)
                    .frame(width: 56, height: 56)
                    .scaleEffect(isAnimating && recordingState == .recording ? 1.1 : 1.0)
                    .animation(
                        recordingState == .recording ? 
                        .easeInOut(duration: 0.8).repeatForever(autoreverses: true) : 
                        .easeInOut(duration: 0.3),
                        value: isAnimating
                    )
                
                // çŠ¶æ€å›¾æ ‡
                Image(systemName: statusIcon)
                    .font(.system(size: 28))
                    .foregroundColor(.white)
                
                // ä¿®å¤çš„å¾½ç« æ•°å­— - ä½¿ç”¨overlayè€ŒéVStackåµŒå¥—
                if shouldShowBadge {
                    Text("\(transcriptWordCount)")
                        .font(.caption2)
                        .fontWeight(.bold)
                        .foregroundColor(.white)
                        .padding(.horizontal, 6)
                        .padding(.vertical, 2)
                        .background(Color.red)
                        .clipShape(Capsule())
                        .offset(x: 20, y: -20)  // ç›¸å¯¹äºCircleçš„åç§»
                }
            }
        }
        .onChange(of: recordingState) { _, newState in
            withAnimation {
                isAnimating = newState == .recording
            }
        }
        .onAppear {
            isAnimating = recordingState == .recording
        }
    }
    
    private var statusColor: Color {
        switch recordingState {
        case .idle, .finished:
            return .blue
        case .recording:
            return .green
        case .paused:
            return .yellow
        case .optimizingTranscription:
            return .orange
        }
    }
    
    private var statusIcon: String {
        switch recordingState {
        case .idle, .finished:
            return "doc.text"
        case .recording:
            return "waveform.and.mic"
        case .paused:
            return "pause.circle"
        case .optimizingTranscription:
            return "gearshape.2"
        }
    }
    
    private var shouldShowBadge: Bool {
        return recordingState == .recording || recordingState == .paused
    }
}

struct NewRecordingView_Previews: PreviewProvider {
    static var previews: some View {
        Group {
            // é»˜è®¤çŠ¶æ€
        NewRecordingView()
                .previewDisplayName("Default State")
            
            // çŠ¶æ€æŒ‡ç¤ºå™¨é¢„è§ˆ - ä¸åŒçŠ¶æ€
            VStack(spacing: 20) {
                HStack(spacing: 20) {
                    TranscriptionStatusIndicator(
                        recordingState: .idle,
                        transcriptWordCount: 0,
                        action: {}
                    )
                    Text("Idle (Blue)")
                }
                
                HStack(spacing: 20) {
                    TranscriptionStatusIndicator(
                        recordingState: .recording,
                        transcriptWordCount: 42,
                        action: {}
                    )
                    Text("Recording (Green)")
                }
                
                HStack(spacing: 20) {
                    TranscriptionStatusIndicator(
                        recordingState: .paused,
                        transcriptWordCount: 28,
                        action: {}
                    )
                    Text("Paused (Yellow)")
                }
                
                HStack(spacing: 20) {
                    TranscriptionStatusIndicator(
                        recordingState: .finished,
                        transcriptWordCount: 0,
                        action: {}
                    )
                    Text("Finished (Blue)")
                }
            }
            .padding()
            .previewDisplayName("Status Indicators")
        }
    }
} 
